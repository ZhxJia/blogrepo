---
title: Deep CRF with Inter-Object Constraints论文阅读
categories:
- 论文
tags:
- track
mathjax: true
---

Deep Continuous Conditional Random Fields With Asymmetric Inter-Object Constraints for Online Multi-Object Tracking

利用了目标之间的非对称关系。

<!--more-->

论文：https://ieeexplore.ieee.org/document/8335792

DCCRF包含两项：

- 一元(unary term)项通过基于深度网络提取得到的视觉外观信息(visual appearance)来估计被跟踪目标随时间的移动

- 异步二元(asymmetric pairwise)以异步的形式建模**物体之间**的联系。

  

### 疑问：

- 文章中提到之前的物体之间关联的建立是通过同步的方式(symmetric way),那么什么算是同步的方式，什么算是异步的方式呢？
  文章中给出的解释如图所示

<img src="deep_crf_tracking\1.png" style="zoom:75%;" />



深度条件随机场所起到的作用到底是什么呢？

###  Movements

- 文章指出之前大部分建立物体间相互作用的模型是基于`symmetric`对称的数学形式,例如描述彼此之间的相互作用是通过一样的衡量标准
- 单个物体的运动以及物体之间的相互作用，在之前的一些工作中大部分是通过人手动去设计模型，而不是通过统一的框架学习得到的

基于此，文章提出`Deep Continuous Conditional Random Field`(DCCRF)建立非对称的物体间的相互作用。对于两个临近的trajectories,high-confidence 的跟踪轨迹应该帮助low-confidence的跟踪轨迹更多一些(即所谓的非对称)。

<img src="deep_crf_tracking\2.png" style="zoom:75%;" />

### How to do it

​	通过向DCCRF中输入两帧（`t,t-1`）连续图像以及到`t-1`时刻被跟踪目标的跟踪轨迹(trajectories),然后估计在`t`时刻被跟踪目标的位置。
​	DCCRF通过两个方面优化目标函数：
​		（1）估计单独个体的运动模式的一元项。通过CNN训练估计从`t-1`到`t`时刻物体的位移`(displacement)`
​		（2）建模被跟踪目标相互作用的非对称二元项。意在解决目标遮挡、目标误识别、相机移动产生的问题。

- 条件随机场简介

- 改进的孪生网络

  - 视觉匹配置信度计算

    将两个图像块在颜色通道上进行级联，具体操作：先裁剪要对比的目标，然后将大小统一为`224*224*3`，在颜色通道上进行级联得到`224*224*6` 输入到网络中，最后计算交叉熵损失。

  - 位移预测网络

    在该框架下，通过CNN预测目标从第`n-1`帧到`n`帧图像的像素位移。对于每一条跟踪链(trajecotry),通过裁剪第`n-1`帧图像和第`n`帧图像，然后在颜色通道上级联送入卷积神经网络，输出即为目标偏移量。

    ![](deep_crf_tracking\3.png)

    如图所示，在已知现有目标位置基础上，裁剪一个高为原始目标的2倍，宽为原始目标5倍的图像块，其中原始目标置于图像块的正中心。在将要跟踪的图像的同样位置裁剪一个同样大小的图像块，然后与上一步得到的图像块在颜色通道进行级联，作为CNN的输入。

- 目标关系分析
  通过高准确度的跟踪链来修正低准确度的跟踪链(trajectory),（小框对于大框的影响应该远大于大框对小框的影响，同理高confidence的跟踪链对于低confidence的跟踪链大于低confidence对高confidence的影响）
                                            ![](deep_crf_tracking\5.png)
  上图中，图(a)展示两个跟踪目标咋i`t-1`时刻的跟踪位置，以及各自的速度。图(b)是其他方法假设的`对称性`约束，图(c)展示了当检测框不准确时，对称性目标关系约束的局限性。图(d)展示了非对称目标关系约束，通过小目标的运动关系修正大部表的错误预测。
  因此，目标关系的建模必须考虑目标关系的不对称性，即单向性。常见的可以反映目标不对称性的信号有：目标大小、目标速度以及目标距离等。同时由检测得到的边界框的**置信度**也可以用于目标关系的建模，**即用高置信度的检测框所匹配的跟踪链来推导低置信度的检测框所匹配的跟踪链。**此时可以利用带置信度的位移预测网络，对目标间的非对称性建模。

- 目标交互建模

  当处理第`n`帧图像时，第`n-1`帧图像中的跟踪链和第`n`帧图像中的检测框输入到整个网络，获得第n帧图像的跟踪链。整个框架由一个CNN构成的一元项和描述目标之间非对称关系的二元项组成。一元项为上述的**位移预测网络**，二元项由目标之间的**速度差、置信度差和面积比例**构成。在这套框架下，利用小框和高置信度的跟踪链去修正大框及低置信度的跟踪链。
  之前的计算还未用到第`n`帧检测的物体，因此在上一步基础上，通过`匹配置信度计算网络`和`匈牙利算法`进行它们之间的数据关联。

  - 总体框架

    基于卷积神经网络的一元项和基于条件随机场的非对称二元项。
                           ![](deep_crf_tracking\6.png)

    **一元项的定义：**假设第`n-1`帧和第`n`帧之间存在`k`个运动的目标，并设这`k`个目标相对于上一帧图像中对应目标中心点的位移为$d=d_1,\dots,d_k$。深度连续条件随机场通过其他目标的位移$(d_j)_{j\neq i}$来优化$d_i$,所以需要较为精准的初始化$\hat{d}=\hat{d_1},\dots,\hat{d_k}$ ,此即为一元项。通过一元项让深度连续条件随机场的输出只能在一元项周围波动，即以位移预测网络的结果为准，但对某些错误的预测进行修正。
    对于第`i`条跟踪链$r_i$,**<u>一元项表示如下</u>**
    $$
    \varphi(d_i,r_i,I)=w_{i,1}(d_i-f_d(r_i,I))^2
    $$
    其中$f_d$为位移预测卷积神经网络，`I`为输入的第`n-1`和`n`帧图像，其中$w_{i,1}$通过下式获得
    $$
    w_{i,1}=\sigma(a_1max(c_i)+b_1)
    $$
    其中$a_1$和$b_1$为权重参数，$c_i$为位置预测卷积神经网络输出的位移置信度。位移预测网络定义了400个位置框，并从左到右依次编号，且最终每个位置框都有一个置信度，论文中采用最大的置信度位置作为唯一预测值。
    **非对称二元项的定义：** 非对称二元项基于目标之间的关系，**用来对目标的预测值做进一步修正**。为了解决全局运动的问题，论文假设第`n-1`帧和第`n`帧的目标速度不变。
    $$
    \psi(d_i,d_j,r_i,r_j,I)=(1-w_{i,1})\sum_k{w_{ij,2}^{(k)}(\vartriangle d_{ij}-\vartriangle s_{ij})}^2
    $$
    其中$\vartriangle d_{ij}=d_i-d_j$为目标`i`和目标`j`在第`n`帧图像的位移差分(可以看作是速度差分)，
    	   $\vartriangle s_{ij}=s_i-s_j$为目标`i`和目标`j`在第`n-1`帧图像的速度差分，
    	   $w_{ij,2}^{(k)}$为一系列可以学习的参数，用来控制不同目标之间非对称性的影响大小。

    由于一般来说大目标的检测框往往比小的检测框催在更大的噪声，因此，小的目标的信息应该对大的目标信息影响大，而反过来大的目标的信息应该对小的目标影响小；位移预测网络中高置信度的位移预测用于修正低置信度的位移预测。
    定义**二元项**的形式如下：
    $$
    w_{ij,2}^{(k)}=\sigma(a_{21}^{(k)}log(area_i/area_j)+b_{21}^{(k)})\times\sigma(a_{22}^{(k)}(max(c_i)-max(c_j))+b_{22}^{(k)})
    $$
    其中，$\sigma$为逻辑回归函数，$area_i$表示第`i`条跟踪链在`n-1`帧图像上的大小。通过位移预测网络$max(c_i)$从${c_i^1,c_i^2,\dots,c_i^m}$中获得最大位移置信度，其中$a_{21}^{(k)},b_{21}^{(k)},a_{22}^{(k)},b_{22}^{(k)}$为可学习的参数。



通过`mean-field`来近似求解全局最大后验边缘概率，**最优位移估计值**是能量函数均值，

### Conclusion





更多概率图模型的内容可参考：
[http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/16_CRF.html](http://www.huaxiaozhuan.com/统计学习/chapters/16_CRF.html)